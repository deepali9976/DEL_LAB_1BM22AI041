import numpy as np
import matplotlib.pyplot as plt


X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])


def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

def tanh(x):
    return np.tanh(x)

def tanh_derivative(x):
    return 1 - np.tanh(x)**2

def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return np.where(x > 0, 1, 0)


class SimpleNN:
    def __init__(self, activation_function='sigmoid'):
        self.input_size = 2
        self.hidden_size = 2
        self.output_size = 1
        
        self.activation_function = activation_function
        
        # Weights initialization
        self.weights_input_hidden = np.random.rand(self.input_size, self.hidden_size)
        self.weights_hidden_output = np.random.rand(self.hidden_size, self.output_size)
    
    def forward(self, X):
        self.hidden_layer_activation = np.dot(X, self.weights_input_hidden)
        self.hidden_layer_output = self.activate(self.hidden_layer_activation)
        self.final_output = np.dot(self.hidden_layer_output, self.weights_hidden_output)
        return self.final_output
    
    def activate(self, x):
        if self.activation_function == 'sigmoid':
            return sigmoid(x)
        elif self.activation_function == 'tanh':
            return tanh(x)
        elif self.activation_function == 'relu':
            return relu(x)
    
    def backpropagate(self, X, y, learning_rate=0.1):

        error = y - self.final_output
        d_final_output = error * self.activate_derivative(self.final_output)


        error_hidden_layer = d_final_output.dot(self.weights_hidden_output.T)
        d_hidden_layer = error_hidden_layer * self.activate_derivative(self.hidden_layer_output)


        self.weights_hidden_output += self.hidden_layer_output.T.dot(d_final_output) * learning_rate
        self.weights_input_hidden += X.T.dot(d_hidden_layer) * learning_rate
    
    def activate_derivative(self, x):
        if self.activation_function == 'sigmoid':
            return sigmoid_derivative(x)
        elif self.activation_function == 'tanh':
            return tanh_derivative(x)
        elif self.activation_function == 'relu':
            return relu_derivative(x)

    def train(self, X, y, epochs=1000):
        for _ in range(epochs):
            self.forward(X)
            self.backpropagate(X, y)
activation_functions = ['sigmoid', 'tanh', 'relu']
results = {}

for activation in activation_functions:
    nn = SimpleNN(activation_function=activation)
    nn.train(X, y)
    

    predictions = nn.forward(X)
    predictions = np.round(predictions).astype(int)
    

    accuracy = np.mean(predictions == y) * 100
    results[activation] = accuracy


print("Accuracy with Different Activation Functions:")
for activation, accuracy in results.items():
    print(f"{activation.capitalize()}: {accuracy:.2f}%")

def plot_activation_functions():
    x = np.linspace(-10, 10, 400)

    plt.figure(figsize=(10, 6))

    plt.subplot(2, 2, 1)
    plt.plot(x, sigmoid(x), label='Sigmoid')
    plt.title('Sigmoid Function')
    plt.grid()

    plt.subplot(2, 2, 2)
    plt.plot(x, tanh(x), label='Tanh', color='orange')
    plt.title('Tanh Function')
    plt.grid()

    plt.subplot(2, 2, 3)
    plt.plot(x, relu(x), label='ReLU', color='green')
    plt.title('ReLU Function')
    plt.grid()

    plt.tight_layout()
    plt.show()

plot_activation_functions()
